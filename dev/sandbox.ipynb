{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_file = (\"/projects/synsight/data/website_data/jump_compounds_matrix.npy\")\n",
    "METADATA_FILE = (\"/projects/synsight/data/website_data/jump_compounds_matrix_metadata.parquet\")\n",
    "output_file = '/projects/synsight/data/website_data/nearest_neighbors.h5'  # Output HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"openphenom\", \"chada\", \"resnet50\", \"dinov2_g\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    npy_file = Path(f'/projects/synsight/data/jump_embeddings/compounds_embeddings/{model}/Embeddings_norm.npy')\n",
    "    all_embeddings[model] = np.load(npy_file)\n",
    "    print(all_embeddings[model].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Charger la matrice depuis le fichier .npy\n",
    "matrix = np.load(DATA_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def worker_sample_chunk(args):\n",
    "    \"\"\"Compute the true cosine similarity for a subset of rows and flatten the results.\"\"\"\n",
    "    row_indices, embeddings = args\n",
    "    \n",
    "    # If embeddings are not guaranteed normalized, we compute norms on the fly\n",
    "    # This can be expensive. Another approach is to ensure embeddings are normalized once at load time.\n",
    "    \n",
    "    # For each row in row_indices, compute dot product with all embeddings\n",
    "    # and divide by norms => true cosine similarity\n",
    "    row_vecs = embeddings[row_indices]  # shape (len(row_indices), D)\n",
    "    row_norms = np.linalg.norm(row_vecs, axis=1, keepdims=True)  # shape (len(row_indices), 1)\n",
    "    all_norms = np.linalg.norm(embeddings, axis=1)  # shape (N,)\n",
    "    \n",
    "    # (len(row_indices), D) dot (D, N) => (len(row_indices), N)\n",
    "    dot_products = np.dot(row_vecs, embeddings.T)\n",
    "    # divide each row by row_norm * all_norms\n",
    "    # row_norms is broadcasted across columns, all_norms is broadcasted across rows\n",
    "    # shape => (len(row_indices), N)\n",
    "    denom = row_norms * all_norms[np.newaxis, :]\n",
    "    cosine_sim = dot_products / denom\n",
    "    \n",
    "    return cosine_sim.flatten()\n",
    "\n",
    "def compute_pairwise_sample(embeddings, sample_indices, n_processes):\n",
    "    \"\"\"\n",
    "    Splits the sampled row indices among n_processes,\n",
    "    computes the cosine similarity in parallel, and returns all pairwise values.\n",
    "    \"\"\"\n",
    "    chunks = np.array_split(sample_indices, n_processes)\n",
    "    args_list = [(chunk, embeddings) for chunk in chunks if len(chunk) > 0]\n",
    "    with mp.Pool(n_processes) as pool:\n",
    "        results = pool.map(worker_sample_chunk, args_list)\n",
    "    # Concatenate all flattened arrays\n",
    "    all_values = np.concatenate(results)\n",
    "    return all_values\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Dictionary for embeddings\n",
    "    all_embeddings = {}\n",
    "    models = [\"openphenom\", \"chada\", \"resnet50\", \"dinov2_g\"]\n",
    "\n",
    "    # Load each .npy file as float16 to reduce memory usage\n",
    "    for model in models:\n",
    "        npy_file = Path(f'/projects/synsight/data/jump_embeddings/compounds_embeddings/{model}/Embeddings_norm.npy')\n",
    "        all_embeddings[model] = np.load(npy_file).astype(np.float32)\n",
    "        # If the file is truly normalized, you could keep it as float16,\n",
    "        # but float32 is safer for the division.\n",
    "\n",
    "    # Number of CPU cores\n",
    "    n_processes = mp.cpu_count()\n",
    "\n",
    "    # Fraction of rows to sample\n",
    "    sample_fraction = 0.001  # 0.1% (adjust as needed for memory/speed tradeoff)\n",
    "\n",
    "    for key, embeddings in tqdm(all_embeddings.items(), desc=\"Models\"):\n",
    "        N = embeddings.shape[0]\n",
    "        sample_size = max(1, int(sample_fraction * N))\n",
    "        sample_indices = np.random.choice(N, size=sample_size, replace=False)\n",
    "\n",
    "        # Compute pairwise *cosine similarities* for these sampled rows\n",
    "        all_values = compute_pairwise_sample(embeddings, sample_indices, n_processes)\n",
    "\n",
    "        # Plot with fewer bins + KDE\n",
    "        plt.figure(figsize=(8, 6), dpi=300)\n",
    "        sns.histplot(all_values, bins=200, kde=True, color='blue', alpha=0.3)\n",
    "        plt.title(f'Distribution of Cosine Similarities for {key}')\n",
    "        plt.xlabel('Cosine Similarity')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'histogram_kde_{key}_linear.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Log-scale version\n",
    "        plt.figure(figsize=(8, 6), dpi=300)\n",
    "        sns.histplot(all_values, bins=200, kde=True, color='blue', alpha=0.3, log_scale=(False, True))\n",
    "        plt.title(f'Distribution of Cosine Similarities (log scale) for {key}')\n",
    "        plt.xlabel('Cosine Similarity')\n",
    "        plt.ylabel('Frequency (log scale)')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'histogram_kde_{key}_log.png', dpi=300)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def worker_sample_chunk(args):\n",
    "    \"\"\"Compute the true cosine similarity for a subset of rows and flatten the results.\"\"\"\n",
    "    row_indices, embeddings = args\n",
    "    row_vecs = embeddings[row_indices]  # shape (len(row_indices), D)\n",
    "    row_norms = np.linalg.norm(row_vecs, axis=1, keepdims=True)  # shape (len(row_indices), 1)\n",
    "    all_norms = np.linalg.norm(embeddings, axis=1)  # shape (N,)\n",
    "    dot_products = np.dot(row_vecs, embeddings.T)  # shape (len(row_indices), N)\n",
    "    denom = row_norms * all_norms[np.newaxis, :]\n",
    "    cosine_sim = dot_products / denom\n",
    "    return cosine_sim.flatten()\n",
    "\n",
    "def compute_pairwise_sample(embeddings, sample_indices, n_processes):\n",
    "    \"\"\"\n",
    "    Splits the sampled row indices among n_processes,\n",
    "    computes the cosine similarity in parallel, and returns all pairwise values.\n",
    "    \"\"\"\n",
    "    chunks = np.array_split(sample_indices, n_processes)\n",
    "    args_list = [(chunk, embeddings) for chunk in chunks if len(chunk) > 0]\n",
    "    with mp.Pool(n_processes) as pool:\n",
    "        results = pool.map(worker_sample_chunk, args_list)\n",
    "    all_values = np.concatenate(results)\n",
    "    return all_values\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Dictionary for embeddings\n",
    "    all_embeddings = {}\n",
    "    models = [\"openphenom\", \"chada\", \"resnet50\", \"dinov2_g\"]\n",
    "\n",
    "    # Load each .npy file as float32 to ensure safe division (even if originally float16)\n",
    "    for model in models:\n",
    "        npy_file = Path(f'/projects/synsight/data/jump_embeddings/compounds_embeddings/{model}/Embeddings_norm.npy')\n",
    "        all_embeddings[model] = np.load(npy_file).astype(np.float32)\n",
    "\n",
    "    # Number of CPU cores for parallel processing\n",
    "    n_processes = mp.cpu_count()\n",
    "\n",
    "    # Fraction of rows to sample (adjust for memory/speed trade-off)\n",
    "    sample_fraction = 0.001  # 0.1%\n",
    "\n",
    "    # Dictionary to store distributions for each model\n",
    "    distributions = {}\n",
    "\n",
    "    # Compute cosine similarity distributions for each model\n",
    "    for key, embeddings in tqdm(all_embeddings.items(), desc=\"Models\"):\n",
    "        N = embeddings.shape[0]\n",
    "        sample_size = max(1, int(sample_fraction * N))\n",
    "        sample_indices = np.random.choice(N, size=sample_size, replace=False)\n",
    "        all_values = compute_pairwise_sample(embeddings, sample_indices, n_processes)\n",
    "        distributions[key] = all_values\n",
    "\n",
    "    ## Create a combined plot for all models (linear scale)\n",
    "    #plt.figure(figsize=(10, 8), dpi=300)\n",
    "    #for model, values in distributions.items():\n",
    "    #    # Plot histogram and KDE for each model; using stat=\"density\" normalizes the histograms\n",
    "    #    sns.histplot(values, bins=200, stat=\"density\", alpha=0.3, kde=True, label=model)\n",
    "    #\n",
    "    #plt.title(\"Distribution of Cosine Similarities for All Models\")\n",
    "    #plt.xlabel(\"Cosine Similarity\")\n",
    "    #plt.ylabel(\"Density\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    #plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(\"histogram_kde_all_models_linear.png\", dpi=300)\n",
    "    #plt.close()\n",
    "\n",
    "    # (Optional) Create a log-scale version for the y-axis\n",
    "    # Create a log-scale plot with filled KDE curves for each model\n",
    "    plt.figure(figsize=(10, 8), dpi=300)\n",
    "    for model, values in distributions.items():\n",
    "        # Plot a filled KDE for each model\n",
    "        sns.kdeplot(values, fill=True, alpha=0.3, label=model)\n",
    "        \n",
    "    # Set y-axis to log scale (note: this applies to the density values computed by the KDE)\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Distribution of Cosine Similarities (log scale) with Filled KDE\")\n",
    "    plt.xlabel(\"Cosine Similarity\")\n",
    "    plt.ylabel(\"Density (log scale)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"filled_kde_all_models_log.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def compute_block(start, end, normalized_embeddings):\n",
    "    \"\"\"\n",
    "    Computes a block of the cosine distance matrix for rows [start, end).\n",
    "    Cosine distance is defined as: distance = 1 - cosine_similarity.\n",
    "    \"\"\"\n",
    "    # Extract the block (shape: block_size x n_features)\n",
    "    block = normalized_embeddings[start:end]\n",
    "    # Compute cosine similarity between the block and all embeddings\n",
    "    # (resulting shape: (block_size, n_samples))\n",
    "    similarity = np.dot(block, normalized_embeddings.T)\n",
    "    # Convert cosine similarity to cosine distance\n",
    "    distances = 1.0 - similarity\n",
    "    # Cast to smallest precision (float16)\n",
    "    return distances.astype(np.float16)\n",
    "\n",
    "def main():\n",
    "    # Load embeddings: shape (n_samples, n_features)\n",
    "    embeddings = all_embeddings[\"dinov2_g\"]\n",
    "    n_samples, _ = embeddings.shape\n",
    "\n",
    "    # Normalize the embeddings (L2 normalization)\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized_embeddings = embeddings / norms\n",
    "\n",
    "    # Choose a block size (adjust as needed to manage memory)\n",
    "    block_size = 100  # For example, 500 rows per block\n",
    "    n_blocks = math.ceil(n_samples / block_size)\n",
    "    \n",
    "    print(f\"Computing distance matrix for {n_samples} samples in {n_blocks} blocks...\")\n",
    "    \n",
    "    # Compute the distance matrix in parallel over blocks\n",
    "    results = Parallel(n_jobs=-1, backend=\"threading\")(\n",
    "        delayed(compute_block)(\n",
    "            start, min(start + block_size, n_samples), normalized_embeddings\n",
    "        )\n",
    "        for start in tqdm(range(0, n_samples, block_size), desc=\"Processing blocks\")\n",
    "    )\n",
    "    \n",
    "    # Stack all computed blocks vertically to form the full matrix\n",
    "    distance_matrix = np.vstack(results)\n",
    "    \n",
    "    # Save the distance matrix as a .npy file with float16 precision\n",
    "    np.save(\"distance_matrix.npy\", distance_matrix)\n",
    "    print(\"Distance matrix saved to 'distance_matrix.npy'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Afficher la distribution des valeurs avec un histogramme\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(all_values, bins=5000, color='blue', alpha=0.3)\n",
    "plt.title('Distribution des valeurs dans la matrice')\n",
    "plt.xlabel('Valeurs')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precompute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000  # Number of nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_parquet(METADATA_FILE)\n",
    "metadata_ids = metadata['Metadata_JCP2022'].values  # Unique molecule IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.load(npy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(i):\n",
    "    \"\"\"\n",
    "    Process a single row of the distance matrix to find the m closest neighbors\n",
    "    and include the distance to a specific molecule.\n",
    "\n",
    "    Args:\n",
    "        i (int): Index of the row in the distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (molecule_id, closest_ids, closest_distances)\n",
    "    \"\"\"\n",
    "    distances = matrix[i]\n",
    "\n",
    "    # Distance to the target molecule (JCP2022_033924)\n",
    "    target_index = np.where(metadata_ids == 'JCP2022_033924')[0][0]\n",
    "    dmso_distance = distances[target_index]\n",
    "\n",
    "    # Find m closest molecules using partial sorting (excluding self if needed)\n",
    "    closest_indices = np.argpartition(distances, m)[1:m+1]  # Top m indices (unsorted)\n",
    "    closest_distances = distances[closest_indices]\n",
    "\n",
    "    # Sort these m indices to ensure proper order\n",
    "    sorted_indices_within_chunk = np.argsort(closest_distances)\n",
    "    closest_indices = closest_indices[sorted_indices_within_chunk]\n",
    "    closest_distances = closest_distances[sorted_indices_within_chunk]\n",
    "\n",
    "    # Get IDs for the closest molecules\n",
    "    closest_ids = metadata_ids[closest_indices]\n",
    "\n",
    "    # Return the results\n",
    "    return metadata_ids[i], closest_ids, closest_distances, dmso_distance\n",
    "\n",
    "# Parallel processing\n",
    "with Pool(processes=cpu_count()) as pool:\n",
    "    # Use tqdm for progress tracking\n",
    "    results = list(tqdm(pool.imap(process_row, range(matrix.shape[0])), total=matrix.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to HDF5\n",
    "with h5py.File(output_file, 'w') as h5f:\n",
    "    for molecule_id, closest_ids, closest_distances, dmso_distance in results:\n",
    "        group = h5f.create_group(molecule_id)\n",
    "        group.create_dataset('closest_ids', data=closest_ids.astype('S'))  # Save IDs as strings\n",
    "        group.create_dataset('distances', data=closest_distances)\n",
    "        group.create_dataset('dmso_distance', data=dmso_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Parameters\n",
    "molecule_id = 'JCP2022_080538'  # Example molecule ID\n",
    "\n",
    "# Access the HDF5 file\n",
    "with h5py.File(output_file, 'r') as h5f:\n",
    "    # Check if the molecule_id exists in the HDF5 file\n",
    "    if molecule_id in h5f:\n",
    "        print(f\"Molecule ID {molecule_id} found.\")\n",
    "        \n",
    "        # Access the datasets\n",
    "        closest_ids = h5f[f'{molecule_id}/dmso_distance'][:].astype(str)  # Convert bytes to strings\n",
    "        distances = h5f[f'{molecule_id}/distances'][:]\n",
    "        \n",
    "        # Print the results\n",
    "        print(f'Closest molecules to {molecule_id}:')\n",
    "        print('IDs:', closest_ids)\n",
    "        print('Distances:', distances)\n",
    "    else:\n",
    "        print(f\"Molecule ID {molecule_id} not found in the HDF5 file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# File path\n",
    "H5_DISTANCE_FILE = output_file\n",
    "\n",
    "# Query molecule ID\n",
    "query_id = \"JCP2022_080538\"  # Replace with the molecule ID you want to query\n",
    "\n",
    "# Open the HDF5 file and retrieve the `dmso_distance`\n",
    "with h5py.File(H5_DISTANCE_FILE, 'r') as h5f:\n",
    "    if query_id in h5f:\n",
    "        dmso_distance = h5f[f\"{query_id}/dmso_distance\"][()]\n",
    "        print(f\"DMSO distance for {query_id}: {dmso_distance}\")\n",
    "    else:\n",
    "        print(f\"Molecule ID {query_id} not found in the HDF5 file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to pg10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add molecular ids (zinc, chembl, pubchem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Metadata_InChI']=='InChI=1S/C2H6OS/c1-4(2)3/h1-2H3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns for metadata (initialize them with default values, e.g., None)\n",
    "data['Zinc_id'] = None  # Replace None with the logic to populate Zinc ids if available\n",
    "data['Canonical_SMILES'] = None  # Replace None with the logic to populate Canonical SMILES if available\n",
    "data['PubChem_id'] = None  # Replace None with the logic to populate PubChem ids if available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "updated_parquet_file_path = \"updated_file.parquet\"  # Replace with your desired output file path\n",
    "data.to_parquet(updated_parquet_file_path, index=False)\n",
    "\n",
    "print(\"Metadata columns added successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
